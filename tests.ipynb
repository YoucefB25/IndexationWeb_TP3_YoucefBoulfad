{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\youce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\youce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\youce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2025-02-08 22:48:14,943 - INFO - Using C:\\Users\\youce\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 22:48:15,126 - WARNING - From c:\\Users\\youce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 22:48:15,214 - WARNING - From c:\\Users\\youce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "2025-02-08 22:48:23,797 - INFO - Fingerprint not found. Saved model loading will continue.\n",
      "2025-02-08 22:48:23,797 - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Constants\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DATA_DIR = \"data\"\n",
    "TITLE_BOOST = 10\n",
    "VARIANT_BOOST = 5\n",
    "BM25_WEIGHT = 0.3\n",
    "SIMILARITY_WEIGHT = 0.7\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load Stopwords and Lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load Universal Sentence Encoder (USE)\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Utility Functions\n",
    "def extract_product_id(url):\n",
    "    \"\"\"Extracts the product ID from a URL.\"\"\"\n",
    "    match = re.search(r\"/product/(\\d+)\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_variant_from_url(url):\n",
    "    \"\"\"Extracts the product variant from the URL.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    return query_params.get(\"variant\", [\"\"])[0]  # Get first variant if exists\n",
    "\n",
    "def get_product_details(url, title_cache={}):\n",
    "    \"\"\"Fetches the product title via scraping and extracts variant from URL.\"\"\"\n",
    "    product_id = extract_product_id(url)\n",
    "    \n",
    "    if product_id in title_cache:\n",
    "        return title_cache[product_id], extract_variant_from_url(url)\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"üîç Scraping: {url}\")\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract title correctly from <h3 class=\"product-title\">\n",
    "        title_element = soup.select_one(\"h3.product-title\")  # Correct selector\n",
    "        if not title_element:\n",
    "            title_element = soup.find(\"meta\", {\"property\": \"og:title\"})  # Fallback\n",
    "        if not title_element:\n",
    "            title_element = soup.find(\"title\")  # Another fallback\n",
    "\n",
    "        # Clean title if extracted from <meta> or <title>\n",
    "        title = title_element.text.strip() if title_element else \"Unknown Title\"\n",
    "\n",
    "        # Cache title to avoid duplicate scrapes\n",
    "        title_cache[product_id] = title  \n",
    "\n",
    "        return title, extract_variant_from_url(url)\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"‚ùå Error fetching {url}: {e}\")\n",
    "    \n",
    "    return \"Unknown Title\", extract_variant_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(title, variant, description, title_boost=TITLE_BOOST, variant_boost=VARIANT_BOOST):\n",
    "    \"\"\"Tokenizes text, giving extra weight to title and variant.\"\"\"\n",
    "    text = f\"{(title + ' ') * title_boost} {(variant + ' ') * variant_boost} {description}\"\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text) if token not in stop_words]\n",
    "\n",
    "def get_sentence_embedding(title, variant, description):\n",
    "    \"\"\"Returns a sentence embedding, emphasizing title and variant.\"\"\"\n",
    "    boosted_text = (title + \" \") * TITLE_BOOST + (variant + \" \") * VARIANT_BOOST + description\n",
    "    embedding = embed([boosted_text]).numpy()[0]\n",
    "    return embedding / np.linalg.norm(embedding)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No product links found on page 1.\n",
      "No product URLs were found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_product_urls(base_url):\n",
    "    product_urls = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}/products?page={page_number}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        product_links = soup.select('a[href^=\"/product/\"]')\n",
    "\n",
    "        if not product_links:\n",
    "            print(f\"No product links found on page {page_number}.\")\n",
    "            break\n",
    "\n",
    "        for link in product_links:\n",
    "            full_url = base_url + link['href']\n",
    "            if full_url not in product_urls:\n",
    "                product_urls.append(full_url)\n",
    "\n",
    "        print(f\"Page {page_number} processed. Found {len(product_links)} product links.\")\n",
    "        page_number += 1\n",
    "\n",
    "    return product_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://web-scraping.dev/products\"\n",
    "    product_urls = scrape_product_urls(base_url)\n",
    "    if product_urls:\n",
    "        print(\"\\nScraped Product URLs:\")\n",
    "        for url in product_urls:\n",
    "            print(url)\n",
    "    else:\n",
    "        print(\"No product URLs were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'description_index.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     75\u001b[0m     json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription_index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m     structured_data, df_counter \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Save structured data\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(json_file)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_data\u001b[39m(json_file):\n\u001b[1;32m---> 41\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     structured_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m     df_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mload_json\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(filename):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'description_index.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def scrape_product_data(url):\n",
    "    \"\"\"Scrapes title, description, and variants from the product page.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        logging.warning(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    title = soup.find(\"h3\", class_=\"card-title product-title\")\n",
    "    description = soup.find(\"p\", class_=\"product-description\")\n",
    "    \n",
    "    # Extract all available variants\n",
    "    variants = [variant.text.strip() for variant in soup.select(\".variants a\")]\n",
    "    \n",
    "    return {\n",
    "        \"title\": title.text.strip() if title else \"\",\n",
    "        \"description\": description.text.strip() if description else \"\",\n",
    "        \"variants\": variants\n",
    "    }\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Splits text into lowercase tokens and counts occurrences.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    return Counter(words)\n",
    "\n",
    "def process_data(json_file):\n",
    "    raw_data = load_json(json_file)\n",
    "    structured_data = []\n",
    "    df_counter = Counter()\n",
    "    \n",
    "    for keyword, url_dict in raw_data.items():\n",
    "        for url, metadata in url_dict.items():\n",
    "            scraped_data = scrape_product_data(url)\n",
    "            if not scraped_data:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize title, description, and variants\n",
    "            title_tokens = tokenize(scraped_data[\"title\"])\n",
    "            description_tokens = tokenize(scraped_data[\"description\"])\n",
    "            variant_tokens = Counter()\n",
    "            for variant in scraped_data[\"variants\"]:\n",
    "                variant_tokens.update(tokenize(variant))\n",
    "            \n",
    "            # Merge all token counts\n",
    "            total_tokens = title_tokens + description_tokens + variant_tokens\n",
    "            df_counter.update(total_tokens)\n",
    "            \n",
    "            structured_data.append({\n",
    "                \"id\": hash(url),\n",
    "                \"title\": scraped_data[\"title\"],\n",
    "                \"variants\": scraped_data[\"variants\"],\n",
    "                \"description\": scraped_data[\"description\"],\n",
    "                \"url\": url,\n",
    "                \"tokens\": dict(total_tokens),\n",
    "            })\n",
    "    \n",
    "    logging.info(f\"Processed {len(structured_data)} products.\")\n",
    "    return structured_data, df_counter\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file = \"description_index.json\"\n",
    "    structured_data, df_counter = process_data(json_file)\n",
    "    \n",
    "    # Save structured data\n",
    "    with open(\"structured_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    # Save token duplication factors\n",
    "    with open(\"token_frequencies.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(df_counter, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Scraping and processing completed.\")\n",
    "    \n",
    "    # Display output for review\n",
    "    print(\"\\nSample Structured Data:\")\n",
    "    print(json.dumps(structured_data[:3], indent=4, ensure_ascii=False))  # Show first 3 products\n",
    "    \n",
    "    print(\"\\nTop 10 Frequent Tokens:\")\n",
    "    for token, count in df_counter.most_common(10):\n",
    "        print(f\"{token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 22:58:21,189 - INFO - Extracted 28 product titles from URLs.\n",
      "2025-02-08 22:58:21,208 - INFO - üîç Scraping: https://web-scraping.dev/product/1\n",
      "2025-02-08 22:58:22,105 - INFO - üîç Scraping: https://web-scraping.dev/product/11\n",
      "2025-02-08 22:58:22,921 - INFO - üîç Scraping: https://web-scraping.dev/product/13?variant=cherry-large\n",
      "2025-02-08 22:58:23,724 - INFO - üîç Scraping: https://web-scraping.dev/product/23\n",
      "2025-02-08 22:58:24,557 - INFO - üîç Scraping: https://web-scraping.dev/product/25\n",
      "2025-02-08 22:58:25,369 - INFO - üîç Scraping: https://web-scraping.dev/product/12\n",
      "2025-02-08 22:58:26,204 - INFO - üîç Scraping: https://web-scraping.dev/product/20\n",
      "2025-02-08 22:58:27,001 - INFO - üîç Scraping: https://web-scraping.dev/product/24\n",
      "2025-02-08 22:58:27,820 - INFO - üîç Scraping: https://web-scraping.dev/product/8?variant=beige-6\n",
      "2025-02-08 22:58:28,786 - INFO - üîç Scraping: https://web-scraping.dev/product/10\n",
      "2025-02-08 22:58:29,598 - INFO - üîç Scraping: https://web-scraping.dev/product/22\n",
      "2025-02-08 22:58:30,555 - INFO - üîç Scraping: https://web-scraping.dev/product/17\n",
      "2025-02-08 22:58:31,363 - INFO - üîç Scraping: https://web-scraping.dev/product/5\n",
      "2025-02-08 22:58:32,201 - INFO - üîç Scraping: https://web-scraping.dev/product/19\n",
      "2025-02-08 22:58:33,047 - INFO - üîç Scraping: https://web-scraping.dev/product/7\n",
      "2025-02-08 22:58:33,878 - INFO - üîç Scraping: https://web-scraping.dev/product/21\n",
      "2025-02-08 22:58:34,677 - INFO - üîç Scraping: https://web-scraping.dev/product/9\n",
      "2025-02-08 22:58:35,526 - INFO - üîç Scraping: https://web-scraping.dev/product/14\n",
      "2025-02-08 22:58:36,345 - INFO - üîç Scraping: https://web-scraping.dev/product/15\n",
      "2025-02-08 22:58:37,183 - INFO - üîç Scraping: https://web-scraping.dev/product/2\n",
      "2025-02-08 22:58:37,990 - INFO - üîç Scraping: https://web-scraping.dev/product/26?variant=one\n",
      "2025-02-08 22:58:38,774 - INFO - üîç Scraping: https://web-scraping.dev/product/27\n",
      "2025-02-08 22:58:39,574 - INFO - üîç Scraping: https://web-scraping.dev/product/3?variant=six-pack\n",
      "2025-02-08 22:58:40,455 - INFO - üîç Scraping: https://web-scraping.dev/product/18\n",
      "2025-02-08 22:58:41,298 - INFO - üîç Scraping: https://web-scraping.dev/product/6?variant=one\n",
      "2025-02-08 22:58:42,286 - INFO - üîç Scraping: https://web-scraping.dev/product/16\n",
      "2025-02-08 22:58:43,086 - INFO - üîç Scraping: https://web-scraping.dev/product/28?variant=one\n",
      "2025-02-08 22:58:43,885 - INFO - üîç Scraping: https://web-scraping.dev/product/4\n",
      "2025-02-08 22:58:45,317 - INFO - Loaded 3483 descriptions.\n",
      "2025-02-08 22:58:45,333 - INFO - Calculated average document length: 39.79816250358886\n",
      "2025-02-08 22:58:50,412 - INFO - üîç Scraping: https://web-scraping.dev/product/1\n",
      "2025-02-08 22:58:51,261 - INFO - üîç Scraping: https://web-scraping.dev/product/11\n",
      "2025-02-08 22:58:52,060 - INFO - üîç Scraping: https://web-scraping.dev/product/13?variant=cherry-large\n",
      "2025-02-08 22:58:52,876 - INFO - üîç Scraping: https://web-scraping.dev/product/23\n",
      "2025-02-08 22:58:53,697 - INFO - üîç Scraping: https://web-scraping.dev/product/25\n",
      "2025-02-08 22:58:54,508 - INFO - üîç Scraping: https://web-scraping.dev/product/12\n",
      "2025-02-08 22:58:55,324 - INFO - üîç Scraping: https://web-scraping.dev/product/20\n",
      "2025-02-08 22:58:56,157 - INFO - üîç Scraping: https://web-scraping.dev/product/24\n",
      "2025-02-08 22:58:56,989 - INFO - üîç Scraping: https://web-scraping.dev/product/8?variant=beige-6\n",
      "2025-02-08 22:58:57,955 - INFO - üîç Scraping: https://web-scraping.dev/product/10\n",
      "2025-02-08 22:58:58,770 - INFO - üîç Scraping: https://web-scraping.dev/product/22\n",
      "2025-02-08 22:58:59,587 - INFO - üîç Scraping: https://web-scraping.dev/product/17\n",
      "2025-02-08 22:59:00,410 - INFO - üîç Scraping: https://web-scraping.dev/product/5\n",
      "2025-02-08 22:59:01,268 - INFO - üîç Scraping: https://web-scraping.dev/product/19\n",
      "2025-02-08 22:59:02,084 - INFO - üîç Scraping: https://web-scraping.dev/product/7\n",
      "2025-02-08 22:59:02,900 - INFO - üîç Scraping: https://web-scraping.dev/product/21\n",
      "2025-02-08 22:59:03,700 - INFO - üîç Scraping: https://web-scraping.dev/product/9\n",
      "2025-02-08 22:59:04,565 - INFO - üîç Scraping: https://web-scraping.dev/product/14\n",
      "2025-02-08 22:59:05,365 - INFO - üîç Scraping: https://web-scraping.dev/product/15\n",
      "2025-02-08 22:59:06,197 - INFO - üîç Scraping: https://web-scraping.dev/product/2\n",
      "2025-02-08 22:59:07,016 - INFO - üîç Scraping: https://web-scraping.dev/product/26?variant=one\n",
      "2025-02-08 22:59:07,824 - INFO - üîç Scraping: https://web-scraping.dev/product/27\n",
      "2025-02-08 22:59:08,612 - INFO - üîç Scraping: https://web-scraping.dev/product/3?variant=six-pack\n",
      "2025-02-08 22:59:09,480 - INFO - üîç Scraping: https://web-scraping.dev/product/18\n",
      "2025-02-08 22:59:10,280 - INFO - üîç Scraping: https://web-scraping.dev/product/6?variant=one\n",
      "2025-02-08 22:59:11,293 - INFO - üîç Scraping: https://web-scraping.dev/product/16\n",
      "2025-02-08 22:59:12,111 - INFO - üîç Scraping: https://web-scraping.dev/product/28?variant=one\n",
      "2025-02-08 22:59:12,975 - INFO - üîç Scraping: https://web-scraping.dev/product/4\n",
      "2025-02-08 22:59:14,612 - INFO - Loaded 3483 descriptions.\n"
     ]
    }
   ],
   "source": [
    "# DataLoader Class\n",
    "class DataLoader:\n",
    "    \"\"\"Loads and processes data from JSON index files.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir=DATA_DIR):\n",
    "        self.data_dir = data_dir\n",
    "        self.df_counter = Counter()\n",
    "        self.title_index = self.load_title_index()\n",
    "        self.documents = self.load_description_index()\n",
    "        self.avgdl = self.calculate_avgdl()\n",
    "        self._precompute_embeddings()  # Precompute embeddings\n",
    "\n",
    "    def _load_json(self, filename):\n",
    "        \"\"\"Loads a JSON file from the data directory.\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            logging.warning(f\"Warning: {filename} not found, returning empty dictionary.\")\n",
    "            return {}\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def load_title_index(self):\n",
    "        \"\"\"Retrieves product titles from URLs or scrapes them if missing.\"\"\"\n",
    "        raw_data = self._load_json(\"description_index.json\")\n",
    "        title_mapping = {}\n",
    "\n",
    "        for keyword, url_dict in raw_data.items():\n",
    "            for url in url_dict.keys():\n",
    "                product_id = extract_product_id(url)\n",
    "                if product_id and product_id not in title_mapping:\n",
    "                    title_mapping[product_id] = get_product_details(url)\n",
    "\n",
    "        logging.info(f\"Extracted {len(title_mapping)} product titles from URLs.\")\n",
    "        return title_mapping\n",
    "\n",
    "    def load_description_index(self):\n",
    "        \"\"\"Loads descriptions and assigns real titles using the title index.\"\"\"\n",
    "        raw_data = self._load_json(\"description_index.json\")\n",
    "        structured_data = []\n",
    "        title_cache = {}\n",
    "\n",
    "        for keyword, url_dict in raw_data.items():\n",
    "            for url, metadata in url_dict.items():\n",
    "                product_id = extract_product_id(url)\n",
    "                product_title, variant = get_product_details(url, title_cache)\n",
    "\n",
    "                tokens = tokenize(product_title, variant, f\"Contains keyword '{keyword}' (score: {metadata[0]}) {url}\")\n",
    "\n",
    "                self.df_counter.update(set(tokens))\n",
    "\n",
    "                structured_data.append({\n",
    "                    \"id\": hash(url),\n",
    "                    \"title\": product_title,\n",
    "                    \"variant\": variant,\n",
    "                    \"description\": f\"Contains keyword '{keyword}' (score: {metadata[0]})\",\n",
    "                    \"url\": url,\n",
    "                    \"tokens\": tokens,\n",
    "                })\n",
    "\n",
    "        logging.info(f\"Loaded {len(structured_data)} descriptions.\")\n",
    "        return structured_data\n",
    "\n",
    "    def calculate_avgdl(self):\n",
    "        \"\"\"Computes the average document length.\"\"\"\n",
    "        total_length = sum(len(doc[\"tokens\"]) for doc in self.documents)\n",
    "        avgdl = total_length / len(self.documents) if self.documents else 0\n",
    "        logging.info(f\"Calculated average document length: {avgdl}\")\n",
    "        return avgdl\n",
    "\n",
    "    def _precompute_embeddings(self):\n",
    "        \"\"\"Precompute embeddings for all documents in parallel.\"\"\"\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(get_sentence_embedding, doc[\"title\"], doc[\"variant\"], doc[\"description\"]): doc for doc in self.documents}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                doc = futures[future]\n",
    "                try:\n",
    "                    doc[\"embedding\"] = future.result()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error computing embedding for {doc['url']}: {e}\")\n",
    "                    \n",
    "DL=DataLoader()\n",
    "documents=DL.load_description_index()\n",
    "#print(documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
